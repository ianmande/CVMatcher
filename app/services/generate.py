import json

import torch
from transformers import AutoModelForCausalLM, AutoTokenizer

model_name = "EleutherAI/gpt-j-6B"
tokenizer = AutoTokenizer.from_pretrained(model_name)

model = AutoModelForCausalLM.from_pretrained(
    model_name, torch_dtype=torch.float16, device_map="auto"
)


def generate_match_analysis(cv_text, job_description):
    prompt = f"""
    Eres un asistente experto en selecci칩n de talento. Compara el siguiente curr칤culum con la oferta de trabajo y devuelve una respuesta **en formato JSON v치lido**.

    游늯 **Curr칤culum del Candidato:**
    {cv_text}

    游늷 **Descripci칩n del Trabajo:**
    {job_description}

    **游늵 Respuesta en formato JSON:**  
    {{
        "skills_match": {{
            "coincidencias": [N칰mero de habilidades coincidentes],
            "total_requeridas": [Total de habilidades requeridas],
            "habilidades_coincidentes": ["React", "React Native", "TypeScript"]
        }},
        "experience_level": {{
            "cumple": [true/false],
            "explicaci칩n": "El candidato tiene m치s de 5 a침os de experiencia en frontend."
        }},
        "recruiter_message": "Hola [Nombre], me emociona postularme para esta vacante..."
    }}
    
    **IMPORTANTE:** Devuelve **exclusivamente** un JSON v치lido sin texto adicional.
    """

    inputs = tokenizer(prompt, return_tensors="pt").to(model.device)
    output = model.generate(**inputs, max_length=800)

    response_text = tokenizer.decode(output[0], skip_special_tokens=True)

    try:
        response_json = json.loads(response_text)
    except json.JSONDecodeError:
        return {
            "error": "LLaMA 2 no gener칩 un JSON v치lido.",
            "raw_response": response_text,
        }

    return response_json
